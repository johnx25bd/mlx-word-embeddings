{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d10fad0-fdcf-45ab-b110-4a4d77fd14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "import random\n",
    "import re\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa783a0-f40b-4b97-b4a9-817dc54ab0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText8:\n",
    "    def __init__(self, min_count=5, batch_size=1000, subsample_threshold=1e-5):\n",
    "        \"\"\"\n",
    "        Initializes the PreprocessText8 class.\n",
    "        Args:\n",
    "        - min_count: Minimum word frequency for vocabulary inclusion.\n",
    "        - batch_size: Number of rows to process in each batch.\n",
    "        - subsample_threshold: Threshold for subsampling frequent words.\n",
    "        \"\"\"\n",
    "        self.min_count = min_count  # Minimum word frequency for vocabulary inclusion\n",
    "        self.batch_size = batch_size  # Batch size for processing data\n",
    "        self.subsample_threshold = subsample_threshold  # Threshold for subsampling frequent words\n",
    "        self.vocab = None  # To store word-to-index mapping\n",
    "        self.word_counts = None  # To store word frequencies\n",
    "\n",
    "    def load_dataset(self, zip_filepath, parquet_filename, text_column):\n",
    "        \"\"\"\n",
    "        Generator to load the dataset from a zipped Parquet file in batches.\n",
    "        Args:\n",
    "        - zip_filepath: Path to the ZIP file containing Parquet files.\n",
    "        - parquet_filename: The specific Parquet file within the ZIP archive.\n",
    "        - text_column: The column in the Parquet file containing the text data.\n",
    "        \"\"\"\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as z:\n",
    "            with z.open(parquet_filename) as f:\n",
    "                # Load the Parquet file into a pandas DataFrame in memory\n",
    "                df = pd.read_parquet(BytesIO(f.read()), engine=\"pyarrow\")\n",
    "\n",
    "                # Process the data in batches\n",
    "                df_iterator = df[text_column].astype(str).values\n",
    "                for i in range(0, len(df_iterator), self.batch_size):\n",
    "                    yield \" \".join(df_iterator[i : i + self.batch_size])\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text by removing punctuation and splitting by spaces.\n",
    "        Args:\n",
    "        - text: Raw text data.\n",
    "        Returns:\n",
    "        - tokens: List of tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\b[a-z]+\\b\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        \"\"\"\n",
    "        Build the vocabulary by counting word frequencies and filtering rare words.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - vocab: Word-to-index mapping.\n",
    "        - word_counts: Word frequencies.\n",
    "        \"\"\"\n",
    "        word_counts = Counter(tokens)\n",
    "        # Filter out words below the minimum count\n",
    "        word_counts = {\n",
    "            word: count\n",
    "            for word, count in word_counts.items()\n",
    "            if count >= self.min_count\n",
    "        }\n",
    "\n",
    "        # Create word-to-index mapping (vocabulary)\n",
    "        vocab = {word: i for i, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "        vocab[\"<UNK>\"] = 0  # Unknown words get a default index of 0\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = word_counts\n",
    "        return vocab, word_counts\n",
    "\n",
    "    def subsample_frequent_words(self, tokens):\n",
    "        \"\"\"\n",
    "        Subsample frequent words based on the subsample_threshold to reduce their frequency.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - subsampled_tokens: List of tokens after subsampling.\n",
    "        \"\"\"\n",
    "        total_count = sum(self.word_counts.values())\n",
    "\n",
    "        # Calculate the subsampling probability for each word\n",
    "        subsample_probs = {\n",
    "            word: 1 - np.sqrt(self.subsample_threshold / (count / total_count))\n",
    "            for word, count in self.word_counts.items()\n",
    "        }\n",
    "\n",
    "        # Subsample the tokens based on their probability\n",
    "        subsampled_tokens = [\n",
    "            word for word in tokens if word not in self.word_counts or np.random.rand() > subsample_probs[word]\n",
    "        ]\n",
    "\n",
    "        return subsampled_tokens\n",
    "\n",
    "    def filter_and_subsample(self, tokens):\n",
    "        \"\"\"\n",
    "        Combines filtering of rare words and subsampling of frequent words.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - processed_tokens: List of tokens after filtering and subsampling.\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary is not set. Please build the vocabulary before filtering.\")\n",
    "    \n",
    "        # Replace rare words with <UNK>\n",
    "        filtered_tokens = [word if word in self.vocab else \"<UNK>\" for word in tokens]\n",
    "    \n",
    "        # Subsample frequent words\n",
    "        subsampled_tokens = self.subsample_frequent_words(filtered_tokens)\n",
    "    \n",
    "        return subsampled_tokens\n",
    "\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert tokenized words to their corresponding indices from the vocabulary.\n",
    "        Args:\n",
    "        - tokens: List of tokens.\n",
    "        Returns:\n",
    "        - indices: List of indices corresponding to tokens.\n",
    "        \"\"\"\n",
    "        indices = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in tokens]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2a5e21-8233-478b-adfa-b96d02ceb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataGenerator:\n",
    "    def __init__(self, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size  # Context window size\n",
    "\n",
    "    def generate_training_pairs(self, text_indices_batch):\n",
    "        \"\"\"\n",
    "        Generates (input, context) pairs for a batch using the skip-gram model.\n",
    "        Args:\n",
    "        - text_indices_batch: List of word indices (batch of text).\n",
    "        Returns:\n",
    "        - pairs: List of (input_word, context_word) pairs.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for i, target_word in enumerate(text_indices_batch):\n",
    "            # Define the context window range\n",
    "            start = max(i - self.window_size, 0)\n",
    "            end = min(i + self.window_size + 1, len(text_indices_batch))\n",
    "\n",
    "            # For each word in the window (except the target word), generate a pair\n",
    "            for context_word in (\n",
    "                text_indices_batch[start:i] + text_indices_batch[i + 1 : end]\n",
    "            ):\n",
    "                pairs.append((target_word, context_word))\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f3a905-6945-454b-b834-cf4fa3ec4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    def __init__(self, preprocessor, data_generator):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.data_generator = data_generator\n",
    "\n",
    "    def process_dataset(self, zip_filepath, parquet_filename, text_column):\n",
    "        # Load the entire dataset\n",
    "        data = next(self.preprocessor.load_dataset(zip_filepath, parquet_filename, text_column))\n",
    "        # Preprocess the text\n",
    "        tokens = self.preprocessor.preprocess_text(data)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, zip_filepath, parquet_filename, text_column):\n",
    "        # Process the dataset to get all tokens\n",
    "        tokens = self.process_dataset(zip_filepath, parquet_filename, text_column)\n",
    "        # Build the vocabulary\n",
    "        vocab, word_counts = self.preprocessor.build_vocab(tokens)\n",
    "        return vocab, word_counts\n",
    "\n",
    "    def filter_and_subsample(self, zip_filepath, parquet_filename, text_column):\n",
    "        # Process the dataset to get all tokens\n",
    "        tokens = self.process_dataset(zip_filepath, parquet_filename, text_column)\n",
    "        # Filter and subsample\n",
    "        filtered_subsampled_tokens = self.preprocessor.filter_and_subsample(tokens)\n",
    "        return filtered_subsampled_tokens\n",
    "\n",
    "    def convert_to_indices(self, tokens):\n",
    "        return self.preprocessor.text_to_indices(tokens)\n",
    "\n",
    "    def generate_training_pairs(self, indices):\n",
    "        return self.data_generator.generate_training_pairs(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b649fadb-29e1-4241-add2-4a95647dc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PreprocessText8 and SkipGramDataGenerator\n",
    "preprocessor = PreprocessText8(min_count=5, subsample_threshold=1e-5)\n",
    "data_generator = SkipGramDataGenerator(vocab=None, window_size=2)\n",
    "\n",
    "# Create an instance of DatasetProcessor\n",
    "processor = DatasetProcessor(preprocessor, data_generator)\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab, word_counts = processor.build_vocab(\"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\")\n",
    "\n",
    "# Filter and subsample the dataset\n",
    "train_filtered_tokens = processor.filter_and_subsample(\"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\")\n",
    "test_filtered_tokens = processor.filter_and_subsample(\"data/test.zip\", \"test-00000-of-00001.parquet\", \"text\")\n",
    "validation_filtered_tokens = processor.filter_and_subsample(\"data/validation.zip\", \"validation-00000-of-00001.parquet\", \"text\")\n",
    "\n",
    "# Convert filtered tokens to indices\n",
    "train_indices = processor.convert_to_indices(train_filtered_tokens)\n",
    "test_indices = processor.convert_to_indices(test_filtered_tokens)\n",
    "validation_indices = processor.convert_to_indices(validation_filtered_tokens)\n",
    "\n",
    "# Generate training pairs for Skip-gram\n",
    "training_pairs = processor.generate_training_pairs(train_indices)\n",
    "test_pairs = processor.generate_training_pairs(test_indices)\n",
    "validation_pairs = processor.generate_training_pairs(validation_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b76ac75b-b13b-43e8-9e2f-071e4302a0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs generated: 17827246\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training pairs generated: {len(training_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed532d-2743-4bef-81d6-8ce9d3284d65",
   "metadata": {},
   "source": [
    "#### 3. Building skip-gram model\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "- Input: One-hot vector of size equal to the vocabulary size.\n",
    "\n",
    "- Hidden Layer: Produces the embedding vector (size embedding_dim).\n",
    "\n",
    "- Output: Softmax over the vocabulary size to predict the context word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7dd5010-d354-4213-abf2-137fbe4d1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding Layer: Converts input words (as indices) to vectors of size embedding_dim. These vectors represent the word embeddings that will be learned.\n",
    "Linear Layer: Maps the embedding vector to a vector of size vocab_size. This represents the probabilities of each word in the vocabulary being a context word.\n",
    "Forward Pass: The forward method defines how input passes through the layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Embedding layer for input (target words)\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Embedding layer for output (context words)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_word, context_word):\n",
    "        # Get embeddings for input and context words\n",
    "        input_embedding = self.input_embeddings(input_word)\n",
    "        context_embedding = self.output_embeddings(context_word)\n",
    "\n",
    "        # Return dot product between input and context embeddings\n",
    "        return torch.sum(input_embedding * context_embedding, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8b2f22-22df-450c-97ec-d779fe736079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling_loss(model, input_word, context_word, negative_samples, device):\n",
    "    \"\"\"\n",
    "    Compute the Negative Sampling loss.\n",
    "    Args:\n",
    "    - model: SkipGramModel instance.\n",
    "    - input_word: Tensor of input word indices.\n",
    "    - context_word: Tensor of positive context word indices.\n",
    "    - negative_samples: Tensor of negative word indices.\n",
    "    - device: Device (CPU/GPU) for computation.\n",
    "    \"\"\"\n",
    "    # Positive samples: calculate log(sigmoid(dot product))\n",
    "    pos_score = model(input_word, context_word)  # Dot product of input and context embeddings\n",
    "    pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "    # Negative samples: calculate log(sigmoid(-dot product))\n",
    "    neg_score = model(input_word.unsqueeze(1), negative_samples)  # Shape: (batch_size, num_neg_samples)\n",
    "    neg_loss = -torch.log(torch.sigmoid(-neg_score)).sum(1)  # Sum over negative samples\n",
    "\n",
    "    # Total loss is the sum of positive and negative losses\n",
    "    total_loss = (pos_loss + neg_loss).mean()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699adbd-8a1b-49e7-9780-b3b77c652a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c7ac3cb-fd1e-4c4d-ab70-d45d40d47636",
   "metadata": {},
   "source": [
    "#### 4. Training the model\n",
    "\n",
    "- Input preparation: Convert the training pairs (input word, context word) into tensors.\n",
    "- Forward pass: For each input word, predict the probability distribution over the context words.\n",
    "- Loss calculation: Use cross-entropy loss to measure how far the predicted probabilities are from the true context word.\n",
    "- Backpropagation: Compute gradients and update model weights using an optimizer.\n",
    "- Repeat for several epochs: Go through the entire dataset multiple times to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d1baaa8-d061-4471-b77c-3b76aa69717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, training_pairs):\n",
    "        self.training_pairs = training_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, context_word = self.training_pairs[idx]\n",
    "        return torch.tensor(input_word), torch.tensor(context_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03972d05-5ff3-4335-bda9-7f514e2f1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipGramModel(\n",
       "  (input_embeddings): Embedding(67428, 200)\n",
       "  (output_embeddings): Embedding(67428, 200)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5\n",
    "embedding_dim = 200\n",
    "learning_rate = 0.1\n",
    "batch_size = 1024\n",
    "\n",
    "# k: negative sampling \n",
    "k = 5\n",
    "vocab_size = len(preprocessor.vocab)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "babc3991-007e-47a0-9694-7c471ef29a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader for training and validation (assuming you have training_pairs and validation_pairs)\n",
    "train_dataset = SkipGramDataset(training_pairs)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=6\n",
    ")\n",
    "\n",
    "validate_dataset = SkipGramDataset(validation_pairs)\n",
    "validate_loader = DataLoader(\n",
    "    validate_dataset, batch_size=batch_size, shuffle=False, num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0626a890-fa1e-443a-84fd-39631b8a3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, validate_loader, loss_function, device, k):\n",
    "    \"\"\"\n",
    "    Evaluate the model using Negative Sampling on the validation set.\n",
    "    Returns:\n",
    "    - Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for input_words, context_words in validate_loader:\n",
    "            input_words, context_words = input_words.to(device), context_words.to(device)\n",
    "            \n",
    "            # Generate negative samples for this batch\n",
    "            negative_samples = torch.randint(0, vocab_size, (input_words.size(0), k)).to(device)\n",
    "            \n",
    "            # Calculate the negative sampling loss\n",
    "            loss = loss_function(model, input_words, context_words, negative_samples, device)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Return the average validation loss\n",
    "    return total_loss / len(validate_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002ad49-e489-4c49-9478-4d4532a4acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with Negative Sampling\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_words, context_words in train_loader:\n",
    "        input_words, context_words = input_words.to(device), context_words.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate negative samples for this batch\n",
    "        negative_samples = torch.randint(0, vocab_size, (input_words.size(0), k)).to(device)\n",
    "\n",
    "        # Calculate the negative sampling loss\n",
    "        loss = negative_sampling_loss(model, input_words, context_words, negative_samples, device)\n",
    "\n",
    "        # Backward pass: Compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    avg_training_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_training_loss}\")\n",
    "\n",
    "    # Optionally, evaluate on the validation dataset after each epoch\n",
    "    avg_validation_loss = evaluate_model(model, validate_loader, negative_sampling_loss, device, k)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b232c49-9f16-47ed-8093-720ccfa0c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb3658-54ac-4da2-8e95-c807e1c65871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b0a2-a92c-43f4-b636-2c88f0cca249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bdd24-2799-4409-93fa-6e3d10cfaf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
