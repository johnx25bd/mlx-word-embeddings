{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "import random\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data loading (from local zip) and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText8:\n",
    "    def __init__(self, min_count=5, batch_size=1000, subsample_threshold=1e-5):\n",
    "        \"\"\"\n",
    "        Initializes the PreprocessText8 class.\n",
    "        Args:\n",
    "        - min_count: Minimum word frequency for vocabulary inclusion.\n",
    "        - batch_size: Number of rows to process in each batch.\n",
    "        - subsample_threshold: Threshold for subsampling frequent words.\n",
    "        \"\"\"\n",
    "        self.min_count = min_count  # Minimum word frequency for vocabulary inclusion\n",
    "        self.batch_size = batch_size  # Batch size for processing data\n",
    "        self.subsample_threshold = subsample_threshold  # Threshold for subsampling frequent words\n",
    "        self.vocab = None  # To store word-to-index mapping\n",
    "        self.word_counts = None  # To store word frequencies\n",
    "\n",
    "    def load_dataset(self, zip_filepath, parquet_filename, text_column):\n",
    "        \"\"\"\n",
    "        Generator to load the dataset from a zipped Parquet file in batches.\n",
    "        Args:\n",
    "        - zip_filepath: Path to the ZIP file containing Parquet files.\n",
    "        - parquet_filename: The specific Parquet file within the ZIP archive.\n",
    "        - text_column: The column in the Parquet file containing the text data.\n",
    "        \"\"\"\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as z:\n",
    "            with z.open(parquet_filename) as f:\n",
    "                # Load the Parquet file into a pandas DataFrame in memory\n",
    "                df = pd.read_parquet(BytesIO(f.read()), engine=\"pyarrow\")\n",
    "\n",
    "                # Process the data in batches\n",
    "                df_iterator = df[text_column].astype(str).values\n",
    "                for i in range(0, len(df_iterator), self.batch_size):\n",
    "                    yield \" \".join(df_iterator[i : i + self.batch_size])\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text by removing punctuation and splitting by spaces.\n",
    "        Args:\n",
    "        - text: Raw text data.\n",
    "        Returns:\n",
    "        - tokens: List of tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\b[a-z]+\\b\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        \"\"\"\n",
    "        Build the vocabulary by counting word frequencies and filtering rare words.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - vocab: Word-to-index mapping.\n",
    "        - word_counts: Word frequencies.\n",
    "        \"\"\"\n",
    "        word_counts = Counter(tokens)\n",
    "        # Filter out words below the minimum count\n",
    "        word_counts = {\n",
    "            word: count\n",
    "            for word, count in word_counts.items()\n",
    "            if count >= self.min_count\n",
    "        }\n",
    "\n",
    "        # Create word-to-index mapping (vocabulary)\n",
    "        vocab = {word: i for i, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "        vocab[\"<UNK>\"] = 0  # Unknown words get a default index of 0\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = word_counts\n",
    "        return vocab, word_counts\n",
    "\n",
    "    def subsample_frequent_words(self, tokens):\n",
    "        \"\"\"\n",
    "        Subsample frequent words based on the subsample_threshold to reduce their frequency.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - subsampled_tokens: List of tokens after subsampling.\n",
    "        \"\"\"\n",
    "        total_count = sum(self.word_counts.values())\n",
    "\n",
    "        # Calculate the subsampling probability for each word\n",
    "        subsample_probs = {\n",
    "            word: 1 - np.sqrt(self.subsample_threshold / (count / total_count))\n",
    "            for word, count in self.word_counts.items()\n",
    "        }\n",
    "\n",
    "        # Subsample the tokens based on their probability\n",
    "        subsampled_tokens = [\n",
    "            word for word in tokens if word not in self.word_counts or np.random.rand() > subsample_probs[word]\n",
    "        ]\n",
    "\n",
    "        return subsampled_tokens\n",
    "\n",
    "    def filter_and_subsample(self, tokens):\n",
    "        \"\"\"\n",
    "        Combines filtering of rare words and subsampling of frequent words.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - processed_tokens: List of tokens after filtering and subsampling.\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary is not set. Please build the vocabulary before filtering.\")\n",
    "    \n",
    "        # Replace rare words with <UNK>\n",
    "        filtered_tokens = [word if word in self.vocab else \"<UNK>\" for word in tokens]\n",
    "    \n",
    "        # Subsample frequent words\n",
    "        subsampled_tokens = self.subsample_frequent_words(filtered_tokens)\n",
    "    \n",
    "        return subsampled_tokens\n",
    "\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert tokenized words to their corresponding indices from the vocabulary.\n",
    "        Args:\n",
    "        - tokens: List of tokens.\n",
    "        Returns:\n",
    "        - indices: List of indices corresponding to tokens.\n",
    "        \"\"\"\n",
    "        indices = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in tokens]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataGenerator:\n",
    "    def __init__(self, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size  # Context window size\n",
    "\n",
    "    def generate_training_pairs(self, text_indices_batch):\n",
    "        \"\"\"\n",
    "        Generates (input, context) pairs for a batch using the skip-gram model.\n",
    "        Args:\n",
    "        - text_indices_batch: List of word indices (batch of text).\n",
    "        Returns:\n",
    "        - pairs: List of (input_word, context_word) pairs.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for i, target_word in enumerate(text_indices_batch):\n",
    "            # Define the context window range\n",
    "            start = max(i - self.window_size, 0)\n",
    "            end = min(i + self.window_size + 1, len(text_indices_batch))\n",
    "\n",
    "            # For each word in the window (except the target word), generate a pair\n",
    "            for context_word in (\n",
    "                text_indices_batch[start:i] + text_indices_batch[i + 1 : end]\n",
    "            ):\n",
    "                pairs.append((target_word, context_word))\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified process_dataset to handle large datasets in batches without filtering\n",
    "def process_dataset_in_batches_no_filter(zip_filepath, parquet_filename, text_column, preprocessor):\n",
    "    \"\"\"\n",
    "    Generalized function to load, preprocess, and process a dataset in batches without filtering.\n",
    "    Args:\n",
    "    - zip_filepath: Path to the ZIP file containing the Parquet file.\n",
    "    - parquet_filename: Name of the Parquet file inside the ZIP archive.\n",
    "    - text_column: The column in the Parquet file containing the text data.\n",
    "    - preprocessor: PreprocessText8 class instance to handle preprocessing.\n",
    "\n",
    "    Yields:\n",
    "    - tokens: List of tokenized words for each batch.\n",
    "    \"\"\"\n",
    "    data_generator = preprocessor.load_dataset(zip_filepath, parquet_filename, text_column)\n",
    "    for batch in data_generator:\n",
    "        tokens = preprocessor.preprocess_text(batch)\n",
    "        yield tokens  # Yield the tokens batch by batch for processing\n",
    "\n",
    "\n",
    "# Process and build vocabulary incrementally without filtering\n",
    "def build_vocab_in_batches(preprocessor, zip_filepath, parquet_filename, text_column):\n",
    "    vocab, word_counts = None, None\n",
    "    data_generator = process_dataset_in_batches_no_filter(zip_filepath, parquet_filename, text_column, preprocessor)\n",
    "\n",
    "    # Accumulate token batches to build the final vocabulary\n",
    "    all_tokens = []\n",
    "    for token_batch in data_generator:\n",
    "        all_tokens.extend(token_batch)\n",
    "\n",
    "    # Build the vocabulary using the accumulated tokens\n",
    "    vocab, word_counts = preprocessor.build_vocab(all_tokens)\n",
    "\n",
    "    # Set the processed vocabulary and word counts in the preprocessor\n",
    "    preprocessor.vocab = vocab\n",
    "    preprocessor.word_counts = word_counts\n",
    "\n",
    "    return vocab, word_counts\n",
    "\n",
    "# Filter and subsample tokens in batches\n",
    "def filter_and_subsample_in_batches(preprocessor, zip_filepath, parquet_filename, text_column):\n",
    "    filtered_data_generator = []\n",
    "    data_generator = process_dataset_in_batches_no_filter(zip_filepath, parquet_filename, text_column, preprocessor)\n",
    "\n",
    "    # Process each batch using the vocabulary set in the preprocessor\n",
    "    for token_batch in data_generator:\n",
    "        processed_tokens = preprocessor.filter_and_subsample(token_batch)\n",
    "        filtered_data_generator.append(processed_tokens)\n",
    "\n",
    "    return filtered_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PreprocessText8 class and SkipGramDataGenerator\n",
    "# preprocessor = PreprocessText8(min_count=5, batch_size=1000)\n",
    "preprocessor = PreprocessText8(min_count=5, batch_size=10, subsample_threshold=1e-5)\n",
    "window_size = 2\n",
    "data_generator = SkipGramDataGenerator(vocab=None, window_size=window_size)\n",
    "\n",
    "# Build vocabulary incrementally in batches\n",
    "vocab, word_counts = build_vocab_in_batches(\n",
    "    preprocessor, \"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\"\n",
    ")\n",
    "\n",
    "# Filter and subsample the data after the vocabulary is built\n",
    "train_filtered_generator = filter_and_subsample_in_batches(\n",
    "    preprocessor, \"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\")\n",
    "\n",
    "test_filtered_generator = filter_and_subsample_in_batches(\n",
    "    preprocessor, \"data/test.zip\", \"test-00000-of-00001.parquet\", \"text\")\n",
    "\n",
    "validate_filtered_generator = filter_and_subsample_in_batches(\n",
    "    preprocessor, \"data/validation.zip\", \"validation-00000-of-00001.parquet\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert filtered token batches to indices\n",
    "def convert_filtered_to_indices_in_batches(filtered_generator, preprocessor):\n",
    "    \"\"\"\n",
    "    Converts filtered token batches to word indices in batches.\n",
    "    Args:\n",
    "    - filtered_generator: Generator of filtered token batches.\n",
    "    - preprocessor: Instance of PreprocessText8 to convert tokens to indices.\n",
    "    Returns:\n",
    "    - indices_generator: List of batches with word indices.\n",
    "    \"\"\"\n",
    "    indices_generator = []\n",
    "    for token_batch in filtered_generator:\n",
    "        indices_batch = preprocessor.text_to_indices(token_batch)\n",
    "        indices_generator.append(indices_batch)\n",
    "\n",
    "    return indices_generator\n",
    "\n",
    "# Generate training pairs in batches from index batches\n",
    "def generate_training_pairs_from_indices(data_generator, indices_batches):\n",
    "    \"\"\"\n",
    "    Generates (input, context) pairs for the Skip-gram model.\n",
    "    Args:\n",
    "    - data_generator: Instance of SkipGramDataGenerator.\n",
    "    - indices_batches: List of index batches to generate pairs from.\n",
    "    Returns:\n",
    "    - training_pairs: List of all (input_word, context_word) pairs.\n",
    "    \"\"\"\n",
    "    training_pairs = []\n",
    "\n",
    "    for text_indices_batch in indices_batches:\n",
    "        batch_pairs = data_generator.generate_training_pairs(text_indices_batch)\n",
    "        training_pairs.extend(batch_pairs)\n",
    "\n",
    "    return training_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating training data\n",
    "\n",
    "In the skip-gram model, for each word (target word), you try to predict the surrounding words (context words) within a window. For example, if the window size is 2, for the sentence [\"Please\", \"mind\", \"the\", \"gap\"], the training pairs would be:\n",
    "\n",
    "(\"mind\", \"Please\"), (\"mind\", \"the\")\n",
    "(\"the\", \"mind\"), (\"the\", \"gap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert filtered and subsampled data to word indices in batches\n",
    "train_indices_generator = convert_filtered_to_indices_in_batches(\n",
    "    train_filtered_generator, preprocessor\n",
    ")\n",
    "test_indices_generator = convert_filtered_to_indices_in_batches(\n",
    "    test_filtered_generator, preprocessor\n",
    ")\n",
    "validate_indices_generator = convert_filtered_to_indices_in_batches(\n",
    "    validate_filtered_generator, preprocessor\n",
    ")\n",
    "\n",
    "# Generate training pairs for SkipGram in batches\n",
    "training_pairs = generate_training_pairs_from_indices(\n",
    "    data_generator, train_indices_generator\n",
    ")\n",
    "test_pairs = generate_training_pairs_from_indices(\n",
    "    data_generator, test_indices_generator\n",
    ")\n",
    "validation_pairs = generate_training_pairs_from_indices(\n",
    "    data_generator, validate_indices_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs generated: 17829410\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training pairs generated: {len(training_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Building skip-gram model\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "- Input: One-hot vector of size equal to the vocabulary size.\n",
    "\n",
    "- Hidden Layer: Produces the embedding vector (size embedding_dim).\n",
    "\n",
    "- Output: Softmax over the vocabulary size to predict the context word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding Layer: Converts input words (as indices) to vectors of size embedding_dim. These vectors represent the word embeddings that will be learned.\n",
    "Linear Layer: Maps the embedding vector to a vector of size vocab_size. This represents the probabilities of each word in the vocabulary being a context word.\n",
    "Forward Pass: The forward method defines how input passes through the layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Embedding layer (hidden layer) with vocab_size and embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Linear layer to project the embedding to vocabulary size\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_words):\n",
    "        \"\"\"\n",
    "        Forward pass for batched input words.\n",
    "        input_words: Tensor of shape [batch_size] containing word indices.\n",
    "        \"\"\"\n",
    "        # Convert input words to their corresponding embeddings\n",
    "        embed = self.embeddings(input_words) \n",
    "\n",
    "        # Pass embeddings through the linear layer to get vocabulary logits\n",
    "        output = self.linear(embed)  \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (logits): tensor([[ 0.0104, -0.8299, -0.3614,  ...,  0.3564,  0.1144,  0.0151],\n",
      "        [ 1.7373, -0.5475, -0.0808,  ...,  0.2547,  0.1420,  1.0908],\n",
      "        [-0.3742,  0.4437, -0.1803,  ...,  0.0894, -0.5240,  0.4222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Logits shape: torch.Size([3, 67428])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sense check the model architecture\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "\n",
    "# Initialize the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Ensure the model is on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = (\n",
    "    nn.CrossEntropyLoss()\n",
    ")  # Softmax + CrossEntropy for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Example of running a forward pass with a batch of input words\n",
    "sample_input = torch.tensor([1, 2, 3], dtype=torch.long, device=device)\n",
    "\n",
    "# Forward pass: Get the logits for the input words\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print out the sample output (logits) and its shape for clarity\n",
    "print(\"Sample output (logits):\", output)\n",
    "print(\"Logits shape:\", output.shape)  # Should be [batch_size, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training the model\n",
    "\n",
    "- Input preparation: Convert the training pairs (input word, context word) into tensors.\n",
    "- Forward pass: For each input word, predict the probability distribution over the context words.\n",
    "- Loss calculation: Use cross-entropy loss to measure how far the predicted probabilities are from the true context word.\n",
    "- Backpropagation: Compute gradients and update model weights using an optimizer.\n",
    "- Repeat for several epochs: Go through the entire dataset multiple times to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, training_pairs):\n",
    "        self.training_pairs = training_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, context_word = self.training_pairs[idx]\n",
    "        return torch.tensor(input_word), torch.tensor(context_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipGramModel(\n",
       "  (embeddings): Embedding(67428, 300)\n",
       "  (linear): Linear(in_features=300, out_features=67428, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 7\n",
    "embedding_dim = 300\n",
    "learning_rate = 0.05\n",
    "batch_size = 1024\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training and validation (assuming you have training_pairs and validation_pairs)\n",
    "train_dataset = SkipGramDataset(training_pairs)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset = SkipGramDataset(validation_pairs)\n",
    "validate_loader = DataLoader(\n",
    "    validate_dataset, batch_size=batch_size, shuffle=False, num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, validate_loader, loss_function, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for input_words, context_words in validate_loader:\n",
    "            input_words, context_words = input_words.to(device), context_words.to(\n",
    "                device\n",
    "            )\n",
    "            output = model(input_words)\n",
    "            loss = loss_function(output, context_words)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(validate_loader)  # Return the average validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "Epoch 1/7, Training Loss: 23.29752178652279\n",
      "Epoch 1/7, Validation Loss: 22.971514416517948\n",
      "Epoch 2/7\n",
      "Epoch 2/7, Training Loss: 23.71898649623283\n",
      "Epoch 2/7, Validation Loss: 22.69840625617327\n",
      "Epoch 3/7\n",
      "Epoch 3/7, Training Loss: 23.69685013558713\n",
      "Epoch 3/7, Validation Loss: 22.806201033946184\n",
      "Epoch 4/7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Average training loss for this epoch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m avg_training_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_words, context_words in train_loader:\n",
    "        input_words, context_words = input_words.to(device), context_words.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Get predictions\n",
    "        output = model(input_words)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, context_words)\n",
    "\n",
    "        # Backward pass: Compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    avg_training_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_training_loss}\")\n",
    "\n",
    "    # Evaluate on the validation dataset after each epoch\n",
    "    avg_validation_loss = evaluate_model(model, validate_loader, loss_function, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/saved/skipgram_word2vec_2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = \"models/saved/skipgram_word2vec_2.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2, model, vocab):\n",
    "    # Get the device where the model's embeddings are located\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Get the word indices\n",
    "    idx1 = vocab.get(word1, vocab[\"<UNK>\"])\n",
    "    idx2 = vocab.get(word2, vocab[\"<UNK>\"])\n",
    "\n",
    "    # Move the indices to the same device as the model\n",
    "    embedding1 = model.embeddings(torch.tensor([idx1], device=device))\n",
    "    embedding2 = model.embeddings(torch.tensor([idx2], device=device))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = F.cosine_similarity(embedding1, embedding2)\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'king' and 'queen': 0.04516332969069481\n"
     ]
    }
   ],
   "source": [
    "# Example: Compare similarity between \"king\" and \"queen\"\n",
    "similarity = cosine_similarity(\"king\", \"queen\", model, vocab)\n",
    "print(f\"Cosine similarity between 'king' and 'queen': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word1, word2, word3, model, vocab):\n",
    "    \"\"\"\n",
    "    Solves the analogy: word1 is to word2 as word3 is to ?\n",
    "    \"\"\"\n",
    "    # Get the device where the model's embeddings are located\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Get word indices\n",
    "    idx1 = vocab.get(word1, vocab[\"<UNK>\"])\n",
    "    idx2 = vocab.get(word2, vocab[\"<UNK>\"])\n",
    "    idx3 = vocab.get(word3, vocab[\"<UNK>\"])\n",
    "\n",
    "    # Move the indices to the same device as the model\n",
    "    embed1 = model.embeddings(torch.tensor([idx1], device=device))\n",
    "    embed2 = model.embeddings(torch.tensor([idx2], device=device))\n",
    "    embed3 = model.embeddings(torch.tensor([idx3], device=device))\n",
    "\n",
    "    # Solve analogy: word1 - word2 + word3\n",
    "    analogy_vector = embed1 - embed2 + embed3\n",
    "\n",
    "    # Find the word closest to analogy_vector\n",
    "    all_embeddings = model.embeddings.weight.to(device)  # Ensure embeddings are on the same device\n",
    "    similarities = F.cosine_similarity(analogy_vector, all_embeddings.unsqueeze(0), dim=2)\n",
    "\n",
    "    # Get the most similar word (excluding input words)\n",
    "    top_match = torch.argmax(similarities, dim=1).item()\n",
    "    for word, idx in vocab.items():\n",
    "        if idx == top_match:\n",
    "            return word\n",
    "\n",
    "    # If no match is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'paris' is to 'france' as 'berlin' is to: paris\n",
      "'doctor' is to 'hospital' as 'teacher' is to: teacher\n",
      "'car' is to 'road' as 'boat' is to: car\n",
      "'coffee' is to 'morning' as 'wine' is to: coffee\n",
      "'pencil' is to 'paper' as 'brush' is to: pencil\n",
      "'cat' is to 'kitten' as 'dog' is to: modern\n",
      "'moon' is to 'night' as 'sun' is to: moon\n",
      "'water' is to 'drink' as 'bread' is to: bread\n",
      "'student' is to 'school' as 'worker' is to: worker\n",
      "'novel' is to 'author' as 'painting' is to: painting\n"
     ]
    }
   ],
   "source": [
    "result = word_analogy(\"paris\", \"france\", \"berlin\", model, vocab)\n",
    "print(f\"'paris' is to 'france' as 'berlin' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"doctor\", \"hospital\", \"teacher\", model, vocab)\n",
    "print(f\"'doctor' is to 'hospital' as 'teacher' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"car\", \"road\", \"boat\", model, vocab)\n",
    "print(f\"'car' is to 'road' as 'boat' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"coffee\", \"morning\", \"wine\", model, vocab)\n",
    "print(f\"'coffee' is to 'morning' as 'wine' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"pencil\", \"paper\", \"brush\", model, vocab)\n",
    "print(f\"'pencil' is to 'paper' as 'brush' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"cat\", \"kitten\", \"dog\", model, vocab)\n",
    "print(f\"'cat' is to 'kitten' as 'dog' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"moon\", \"night\", \"sun\", model, vocab)\n",
    "print(f\"'moon' is to 'night' as 'sun' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"water\", \"drink\", \"bread\", model, vocab)\n",
    "print(f\"'water' is to 'drink' as 'bread' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"student\", \"school\", \"worker\", model, vocab)\n",
    "print(f\"'student' is to 'school' as 'worker' is to: {result}\")\n",
    "\n",
    "result = word_analogy(\"novel\", \"author\", \"painting\", model, vocab)\n",
    "print(f\"'novel' is to 'author' as 'painting' is to: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'anarchism': [('anarchism', np.float32(1.0000001)), ('tournaments', np.float32(0.33770555)), ('private', np.float32(0.30305058)), ('metals', np.float32(0.30206954)), ('honor', np.float32(0.30183873))]\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute the cosine similarity between the word embeddings\n",
    "\n",
    "\n",
    "def get_word_embedding(word, model, vocab):\n",
    "    # Get the index of the word\n",
    "    idx = vocab.get(word, vocab[\"<UNK>\"])\n",
    "    # Extract the embedding for the word and flatten it to 1D\n",
    "    embedding = model.embeddings(torch.tensor([idx])).squeeze()\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Ensure both vectors are 1D arrays\n",
    "    vec1 = vec1.flatten()\n",
    "    vec2 = vec2.flatten()\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "def find_similar_words(word, model, vocab, top_n=5):\n",
    "    word_embedding = get_word_embedding(word, model, vocab)\n",
    "    similarities = {}\n",
    "\n",
    "    for other_word in vocab:\n",
    "        other_embedding = get_word_embedding(other_word, model, vocab)\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities[other_word] = similarity\n",
    "\n",
    "    # Sort by similarity and return the top_n words\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[\n",
    "        :top_n\n",
    "    ]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find words most similar to \"anarchism\"\n",
    "similar_words = find_similar_words(\"king\", model, vocab)\n",
    "print(f\"Words most similar to 'king': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps\n",
    "# train on whole dataset\n",
    "# evaluate results\n",
    "# fine tune on titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
