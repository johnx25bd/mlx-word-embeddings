{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UdWA3l44_EO",
    "outputId": "6592e7dd-9d86-4761-9f85-4579cca6f9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.16.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (69.0.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I6V7vEgT6luR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maskarkg12\u001b[0m (\u001b[33maskarkg12-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "16680599\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "with open(\"text8\") as f:\n",
    "    text8: str = f.read()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" <PERIOD> \")\n",
    "    text = text.replace(\",\", \" <COMMA> \")\n",
    "    text = text.replace('\"', \" <QUOTATION_MARK> \")\n",
    "    text = text.replace(\";\", \" <SEMICOLON> \")\n",
    "    text = text.replace(\"!\", \" <EXCLAMATION_MARK> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    text = text.replace(\"(\", \" <LEFT_PAREN> \")\n",
    "    text = text.replace(\")\", \" <RIGHT_PAREN> \")\n",
    "    text = text.replace(\"--\", \" <HYPHENS> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    text = text.replace(\":\", \" <COLON> \")\n",
    "    words = text.split()\n",
    "    stats = collections.Counter(words)\n",
    "    words = [word for word in words if stats[word] > 5]\n",
    "    return words\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "corpus: list[str] = preprocess(text8)\n",
    "print(type(corpus))  # <class 'list'>\n",
    "print(len(corpus))  # 16,680,599\n",
    "print(corpus[:7])  # ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    word_counts = collections.Counter(words)\n",
    "    vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "    int_to_vocab = {ii + 1: word for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab[0] = \"<PAD>\"\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab, word_counts\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "words_to_ids, ids_to_words, word_counts = create_lookup_tables(corpus)\n",
    "tokens = [words_to_ids[word] for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06363056866243233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[\"the\"] / len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SGE1vDuFCdgK"
   },
   "outputs": [],
   "source": [
    "def get_context_words(words, center_id, context_window):\n",
    "    total_len = len(words)\n",
    "    start = max(0, center_id)\n",
    "    end = min(total_len, center_id + context_window + 1)\n",
    "    return words[start:i] + words[i + 1 : end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h_QKJnPgCgKd"
   },
   "outputs": [],
   "source": [
    "import numpy.random as random\n",
    "\n",
    "\n",
    "def negative_pair_generator(words, context_window, number_of_samples):\n",
    "    total_len = len(words)\n",
    "    for i, word in enumerate(words):\n",
    "        neg_samples = []\n",
    "        for i in range(number_of_samples):\n",
    "            sample_index = i\n",
    "            while (\n",
    "                sample_index > i - context_window and sample_index < i + context_window\n",
    "            ):\n",
    "                sample_index = random.randint(0, total_len - 1)\n",
    "            neg_samples.append(words[sample_index])\n",
    "        for neg_sample in neg_samples:\n",
    "            yield word, neg_sample\n",
    "\n",
    "\n",
    "def get_negative_samples(words, center_id, context_window, number_of_samples):\n",
    "    neg_samples = []\n",
    "    for i in range(number_of_samples):\n",
    "        sample_index = i\n",
    "        while sample_index > i - context_window and sample_index < i + context_window:\n",
    "            sample_index = random.randint(0, total_len - 1)\n",
    "        neg_samples.append(words[sample_index])\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lbPNa9DPCojM"
   },
   "outputs": [],
   "source": [
    "training_split_ratio = 0.8\n",
    "traing_test_cutoff = int(len(corpus) * training_split_ratio)\n",
    "training_words = corpus[:traing_test_cutoff]\n",
    "test_words = corpus[traing_test_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LtCiqjhgC5Vp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[43mtraining_pairs\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(next(training_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeYS5o5hC-ti"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.center_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_projection_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def similarity(self, ids1, ids2):\n",
    "        if ids1 is int:\n",
    "            ids1 = torch.tensor(ids1)\n",
    "        if ids2 is int:\n",
    "            ids2 = torch.tensor(ids2)\n",
    "\n",
    "        center_embed = self.center_embed(ids1)\n",
    "        context_proj_embed = self.context_proj_embed(ids2)\n",
    "        dot_product = torch.matmul(center_embed, context_proj_embed).sum(1)\n",
    "        return dot_product\n",
    "\n",
    "    def forward(self, id):\n",
    "        if id is not torch.Tensor:\n",
    "            id = torch.tensor(id)\n",
    "        return self.center_embed(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YKmm40lENRj"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCw6VzyLFCiF"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "\n",
    "def generate_batch(generator, batch_size):\n",
    "    result = list(islice(generator, batch_size))\n",
    "    exhausted = len(result) < batch_size\n",
    "    return result, exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampling_threshold = 10**-5\n",
    "\n",
    "\n",
    "def word_seq_generator_enumerated(words):\n",
    "    for i, word in enumerate(words):\n",
    "        word_freq = word_counts[word] / len(corpus)\n",
    "        drop_prob = np.sqrt(subsampling_threshold / word_freq)\n",
    "        if random.ranf() < drop_prob:\n",
    "            continue\n",
    "        yield i, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "39Bp6qiVDqfN",
    "outputId": "9889782e-5937-4e6a-e346-630c45deab2a"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "batch_size = 3_000\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "neg_sample_count = 20\n",
    "\n",
    "context_window = 2\n",
    "\n",
    "samples_per_words = neg_sample_count + 2 * context_window\n",
    "\n",
    "words_per_batch = np.floor(batch_size / samples_per_words).astype(int)\n",
    "\n",
    "model = Word2Vec(embedding_dim=embedding_dim, vocab_size=total_vocab_size).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "wandb.init(project=\"word2vec\")\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    word_seq = word_seq_generator_enumerated(words)\n",
    "\n",
    "    exhausted = False\n",
    "\n",
    "    batch_counter = 0\n",
    "\n",
    "    while not exhausted:\n",
    "        batch_counter += 1\n",
    "\n",
    "        batch_inputs_list = []\n",
    "        batch_labels_list = []\n",
    "        for _ in range(words_per_batch):\n",
    "            # Select word to train\n",
    "            center_words = [\n",
    "                next(word_seq_generator_enumerated) for _ in range(words_per_batch)\n",
    "            ]\n",
    "\n",
    "            for index, center_word in center_words:\n",
    "                # Get all context words\n",
    "                context_words = get_context_words(\n",
    "                    words, words_to_ids[center_word], context_window\n",
    "                )\n",
    "\n",
    "                pos_pair_inputs = torch.tensor(\n",
    "                    [\n",
    "                        [words_to_ids[center_word], words_to_ids[context_word]]\n",
    "                        for context_word in context_words\n",
    "                    ]\n",
    "                )\n",
    "                pos_labels = torch.ones_like(pos_pair_inputs)\n",
    "\n",
    "                # Get all negative samples\n",
    "                neg_words = get_negative_samples(\n",
    "                    words, words_to_ids[center_word], context_window, neg_sample_count\n",
    "                )\n",
    "\n",
    "                neg_pair_inputs = torch.tensor(\n",
    "                    [\n",
    "                        [words_to_ids[center_word], words_to_ids[context_word]]\n",
    "                        for context_word in neg_words\n",
    "                    ]\n",
    "                )\n",
    "                neg_labels = torch.zeros_like(neg_pair_inputs)\n",
    "\n",
    "                word_pairs = torch.cat(pos_pair_inputs)\n",
    "                word_labels = torch.cat(pos_labels, neg_labels)\n",
    "\n",
    "                batch_inputs_list.append(word_pairs)\n",
    "                batch_labels_list.append(word_labels)\n",
    "\n",
    "        # Batch everything\n",
    "        batch_inputs = torch.cat(batch_inputs_list)\n",
    "        batch_labels = torch.cat(batch_labels_list)\n",
    "\n",
    "        pred = model.similarity(batch_inputs)\n",
    "\n",
    "        loss = criterion(pred, batch_labels)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PnjLzw2H4jq"
   },
   "outputs": [],
   "source": [
    "batch, exhausted = generate_batch(training_pairs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUHbZo7QH9Dh"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([word_to_id[word] for word, _ in batch])\n",
    "target_ids = torch.tensor([word_to_id[word] for _, word in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6dmzy8DMHaV",
    "outputId": "d330c9a2-08e3-4cac-ae10-7929382d3776"
   },
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FS-1e1EhIUsr",
    "outputId": "5e787c11-0042-475d-da4d-c8d42c770528"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(embedding_dim=embedding_dim, vocab_size=total_vocab_size)\n",
    "model.similarity(input_ids, target_ids)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
