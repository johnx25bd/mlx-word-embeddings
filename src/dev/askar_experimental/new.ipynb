{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import collections\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "with open(\"text8\") as f:\n",
    "    text8: str = f.read()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" <PERIOD> \")\n",
    "    text = text.replace(\",\", \" <COMMA> \")\n",
    "    text = text.replace('\"', \" <QUOTATION_MARK> \")\n",
    "    text = text.replace(\";\", \" <SEMICOLON> \")\n",
    "    text = text.replace(\"!\", \" <EXCLAMATION_MARK> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    text = text.replace(\"(\", \" <LEFT_PAREN> \")\n",
    "    text = text.replace(\")\", \" <RIGHT_PAREN> \")\n",
    "    text = text.replace(\"--\", \" <HYPHENS> \")\n",
    "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "    text = text.replace(\":\", \" <COLON> \")\n",
    "    words = text.split()\n",
    "    stats = collections.Counter(words)\n",
    "    words = [word for word in words if stats[word] > 5]\n",
    "    return words\n",
    "\n",
    "\n",
    "corpus: list[str] = preprocess(text8)\n",
    "\n",
    "\n",
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    word_counts = collections.Counter(words)\n",
    "    vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "    int_to_vocab = {ii + 1: word for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab[0] = \"<PAD>\"\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab, word_counts\n",
    "\n",
    "\n",
    "words_to_ids, ids_to_words, word_counts = create_lookup_tables(corpus)\n",
    "tokens = [words_to_ids[word] for word in corpus]\n",
    "\n",
    "total_vocab_size = len(corpus)\n",
    "\n",
    "\n",
    "def get_context_words(words, center_id, context_window):\n",
    "    total_len = len(words)\n",
    "    start = max(0, center_id)\n",
    "    end = min(total_len, center_id + context_window + 1)\n",
    "    return words[start:center_id] + words[center_id + 1 : end]\n",
    "\n",
    "\n",
    "def negative_pair_generator(words, context_window, number_of_samples):\n",
    "    total_len = len(words)\n",
    "    for i, word in enumerate(words):\n",
    "        neg_samples = []\n",
    "        for i in range(number_of_samples):\n",
    "            sample_index = i\n",
    "            while (\n",
    "                sample_index > i - context_window and sample_index < i + context_window\n",
    "            ):\n",
    "                sample_index = random.randint(0, total_len - 1)\n",
    "            neg_samples.append(words[sample_index])\n",
    "        for neg_sample in neg_samples:\n",
    "            yield word, neg_sample\n",
    "\n",
    "\n",
    "def get_negative_samples(words, center_id, context_window, number_of_samples):\n",
    "    total_len = len(words)\n",
    "    neg_samples = []\n",
    "    for i in range(number_of_samples):\n",
    "        sample_index = center_id\n",
    "        while sample_index > i - context_window and sample_index < i + context_window:\n",
    "            sample_index = random.randint(0, total_len - 1)\n",
    "        neg_samples.append(words[sample_index])\n",
    "    return neg_samples\n",
    "\n",
    "\n",
    "training_split_ratio = 0.8\n",
    "traing_test_cutoff = int(len(corpus) * training_split_ratio)\n",
    "training_words = corpus[:traing_test_cutoff]\n",
    "test_words = corpus[traing_test_cutoff:]\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.center_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_projection_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def similarity(self, ids1, ids2):\n",
    "        if ids1 is int:\n",
    "            ids1 = torch.tensor(ids1)\n",
    "        if ids2 is int:\n",
    "            ids2 = torch.tensor(ids2)\n",
    "\n",
    "        center_embed = self.center_embed(ids1)\n",
    "        context_proj_embed = self.context_proj_embed(ids2)\n",
    "        dot_product = torch.matmul(center_embed, context_proj_embed).sum(1)\n",
    "        return dot_product\n",
    "\n",
    "    def forward(self, id):\n",
    "        if id is not torch.Tensor:\n",
    "            id = torch.tensor(id)\n",
    "        return self.center_embed(id)\n",
    "\n",
    "\n",
    "# %%\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def generate_batch(generator, batch_size):\n",
    "    result = list(islice(generator, batch_size))\n",
    "    exhausted = len(result) < batch_size\n",
    "    return result, exhausted\n",
    "\n",
    "\n",
    "# %%\n",
    "subsampling_threshold = 10**-5\n",
    "\n",
    "\n",
    "def word_seq_generator_enumerated(words):\n",
    "    for i, word in enumerate(words):\n",
    "        word_freq = word_counts[word] / len(corpus)\n",
    "        drop_prob = np.sqrt(subsampling_threshold / word_freq)\n",
    "        if random.ranf() < drop_prob:\n",
    "            continue\n",
    "        yield i, word\n",
    "\n",
    "\n",
    "# %%\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 3_000\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "neg_sample_count = 20\n",
    "\n",
    "context_window = 2\n",
    "\n",
    "samples_per_words = neg_sample_count + 2 * context_window\n",
    "\n",
    "words_per_batch = np.floor(batch_size / samples_per_words).astype(int)\n",
    "\n",
    "model = Word2Vec(embedding_dim=embedding_dim, vocab_size=total_vocab_size).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "wandb.init(project=\"word2vec\")\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    word_seq = word_seq_generator_enumerated(corpus)\n",
    "\n",
    "    exhausted = False\n",
    "\n",
    "    batch_counter = 0\n",
    "\n",
    "    while not exhausted:\n",
    "        batch_counter += 1\n",
    "\n",
    "        batch_inputs_list = []\n",
    "        batch_labels_list = []\n",
    "        for _ in range(words_per_batch):\n",
    "            # Select word to train\n",
    "            index, center_word = next(word_seq_generator_enumerated)\n",
    "\n",
    "            context_words = get_context_words(\n",
    "                corpus, words_to_ids[center_word], context_window\n",
    "            )\n",
    "\n",
    "            pos_pair_inputs = torch.tensor(\n",
    "                [\n",
    "                    [words_to_ids[center_word], words_to_ids[context_word]]\n",
    "                    for context_word in context_words\n",
    "                ]\n",
    "            )\n",
    "            pos_labels = torch.ones_like(pos_pair_inputs)\n",
    "\n",
    "            # Get all negative samples\n",
    "            neg_words = get_negative_samples(\n",
    "                corpus, words_to_ids[center_word], context_window, neg_sample_count\n",
    "            )\n",
    "\n",
    "            neg_pair_inputs = torch.tensor(\n",
    "                [\n",
    "                    [words_to_ids[center_word], words_to_ids[context_word]]\n",
    "                    for context_word in neg_words\n",
    "                ]\n",
    "            )\n",
    "            neg_labels = torch.zeros_like(neg_pair_inputs)\n",
    "\n",
    "            word_pairs = torch.cat(pos_pair_inputs)\n",
    "            word_labels = torch.cat(pos_labels, neg_labels)\n",
    "\n",
    "            batch_inputs_list.append(word_pairs)\n",
    "            batch_labels_list.append(word_labels)\n",
    "\n",
    "        # Batch everything\n",
    "        batch_inputs = torch.cat(batch_inputs_list)\n",
    "        batch_labels = torch.cat(batch_labels_list)\n",
    "\n",
    "        pred = model.similarity(batch_inputs)\n",
    "\n",
    "        loss = criterion(pred, batch_labels)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
